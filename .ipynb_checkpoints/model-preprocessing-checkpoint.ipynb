{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc4c0875",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer, regexp_tokenize, word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "import sys\n",
    "import os\n",
    "import string\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from utils import dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2476ccb6",
   "metadata": {},
   "source": [
    "### Using the included data importer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99c5090b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset\n",
      "Total stances: 49972\n",
      "Total bodies: 1683\n"
     ]
    }
   ],
   "source": [
    "train_data = dataset.DataSet(name='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67dc6589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c88b88",
   "metadata": {},
   "source": [
    "### Converting imported data into pandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2497417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hundreds of Palestinians were evacuated from their homes Sunday morning after Israeli authorities opened a number of dams near the border, flooding the Gaza Valley in the wake of a recent severe winter storm.\n",
      "\n",
      "The Gaza Ministry of Interior said in a statement that civil defense services and teams from the Ministry of Public Works had evacuated more than 80 families from both sides of the Gaza Valley (Wadi Gaza) after their homes flooded as water levels reached more than three meters.\n",
      "\n",
      "Gaza has experienced flooding in recent days amid a major storm that saw temperatures drop and frigid rain pour down.\n",
      "\n",
      "The storm displaced dozens and caused hardship for tens of thousands, including many of the approximately 110,000 Palestinians left homeless by Israel's assault over summer.\n",
      "\n",
      "The suffering is compounded by the fact that Israel has maintained a complete siege over Gaza for the last eight years, severely limiting electricity and the availability of fuel for generators. It has also prevented the displaced from rebuilding their homes, as construction materials are largely banned from entering.\n",
      "\n",
      "Gaza civil defense services spokesman Muhammad al-Midana warned that further harm could be caused if Israel opens up more dams in the area, noting that water is currently flowing at a high speed from the Israel border through the valley and into the Mediterranean sea.\n",
      "\n",
      "Evacuated families have been sent to shelters sponsored by UNRWA, the UN agency for Palestinian refugees, in al-Bureij refugee camp and in al-Zahra neighborhood in the central Gaza Strip.\n",
      "\n",
      "The Gaza Valley (Wadi Gaza) is a wetland located in the central Gaza Strip between al-Nuseirat refugee camp and al-Moghraqa. It is called HaBesor in Hebrew, and it flows from two streams -- one whose source runs from near Beersheba, and the other from near Hebron.\n",
      "\n",
      "Israeli dams on the river to collect rainwater have dried up the wetlands inside Gaza, and destroyed the only source of surface water in the area.\n",
      "\n",
      "Locals have continued to use it to dispose of their waste for lack of other ways to do so, however, creating an environmental hazard.\n",
      "\n",
      "This is not the first time Israeli authorities have opened the Gaza Valley dams.\n",
      "\n",
      "In Dec. 2013, Israeli authorities also opened the dams amid heavy flooding in the Gaza Strip. The resulting floods damaged dozens of homes and forces many families in the area from their homes.\n",
      "\n",
      "In 2010, the dams were opened as well, forcing 100 families from their homes. At the time civil defense services said that they had managed to save seven people who had been at risk of drowning.\n",
      "\n",
      "'Hairgate': iPhone 6 users encounter latest Apple problem after 'bendgate'\n",
      "\n",
      "unrelated\n"
     ]
    }
   ],
   "source": [
    "train_data_articles_df = pd.DataFrame({'Article': train_data.articles.values(), 'Body ID':train_data.articles.keys()} , index=train_data.articles.keys())\n",
    "train_data_stances_df = pd.DataFrame(train_data.stances)\n",
    "\n",
    "# checking out example output\n",
    "print(train_data_articles_df['Article'][158] + '\\n')\n",
    "print(train_data_stances_df['Headline'][158] + '\\n')\n",
    "print(train_data_stances_df['Stance'][158])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d38444",
   "metadata": {},
   "source": [
    "### Preprocessing: Tokenize and remove stopwords. \n",
    "\n",
    "Can do more, obvious ones: remove news agency header, remove website URL, remove twitter usernames, stemming, lemmatizating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae78dca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\b[^\\d\\W]+\\b') # regex can be improved but idk\n",
    "train_data_articles_df['article_cleaned'] = train_data_articles_df['Article'].apply(lambda x: tokenizer.tokenize(x.lower()))\n",
    "train_data_stances_df['headline_cleaned'] = train_data_stances_df['Headline'].apply(lambda x: tokenizer.tokenize(x.lower()))\n",
    "\n",
    "# remove_whitespace = r'\\s+'\n",
    "# train_data_articles_df['article_cleaned'] = train_data_articles_df['Article'].apply(lambda x: re.split(remove_whitespace, x))\n",
    "# train_data_stances_df['headline_cleaned'] = train_data_stances_df['Headline'].apply(lambda x: re.split(remove_whitespace, x))\n",
    "\n",
    "# exclude = r'[^/d/W]+'\n",
    "# train_data_articles_df['article_cleaned'] = train_data_articles_df['article_cleaned'].apply(lambda x: re.findall(exclude, x))\n",
    "# train_data_stances_df['headline_cleaned'] = train_data_stances_df['headline_cleaned'].apply(lambda x: re.findall(exclude, x))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d3f605e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thousands of people have been duped by a fake news story claiming that Nasa has forecast a total blackout of earth for six days in December.\n",
      "\n",
      "The story, entitled \"Nasa Confirms Earth Will Experience 6 Days of Total Darkness in December 2014!\" originated from Huzlers.com, a website well known for publishing fake stories with sensational headlines.\n",
      "\n",
      "The bogus report read: \"Nasa has confirmed that the Earth will experience 6 days of almost complete darkness and will happen from the dates Tuesday the 16 – Monday the 22 in December. The world will remain, during these three days, without sunlight due to a solar storm, which will cause dust and space debris to become plentiful and thus, block 90% sunlight.\n",
      "\n",
      "\"The head of Nasa Charles Bolden who made the announcement and asked everyone to remain calm. This will be the product of a solar storm, the largest in the last 250 years for a period of 216 hours total.\n",
      "\n",
      "\"Despite the six days of darkness soon to come, officials say that the earth will not experience any major problems, since six days of darkness is nowhere near enough to cause major damage to anything.\"\n",
      "\n",
      "Adding on, the article also carried a made-up quote from Nasa scientist Earl Godoy, saying: \"We will solely rely on artificial light for the six days, which is not a problem at all.\"\n",
      "\n",
      "Many Twitter users believed the fake news report, and expressed their shock.\n",
      "\n",
      "We're going to have a complete 6 days of darkness due to a solar storm in Dec! SO NERVOUS ABOUT THIS! Ahhh. #ThePurge http://t.co/0L2Sis54hv— Janella (@hijanellamarie) October 26, 2014 6 days of total darkness in December? ? http://t.co/eTN60TnXft— Jammie Macaranas (@JammiePeach) October 26, 2014 \"NASA Confirms Earth will experience 6 Days of total DARKNESS in December 2014.\" Me: pic.twitter.com/xZG1xaxqdw— [Hiatus] TT (@sarangBCES) October 26, 2014 \"NASA Confirms Earth Will Experience 6 Days of Total Darkness in December 2014!\" omg what?— 查理 (@Chxrliecutie) October 26, 2014 islam know what this means im scared \"NASA Confirms Earth Will Experience 6 Days of Total Darkness in December 2014! http://t.co/GQGeGLmElZ\"— hiatus (@taobby) October 26, 2014\n",
      "\n",
      "The website has previously published a fake report about American rapper and actor Tupac Shakur, claiming that he is alive.\n",
      "\n",
      "RelatedHalloween 2014 on Friday the 13th for First Time in 666 Years Declared a HoaxShah Rukh Khan's Son Aryan and Aishwarya Rai Bachchan's Niece Navya's Leaked Sex Tape is FakeEbola Zombies: Victims 'Rising from the Dead' Fake News Story Goes Viral, Sparks Outrage on Social MediaEminem 'Quits Music After Checking Into Rehab Again For Heroin Addiction' is Hoax: Satirical Article Creates Stir on Social Media\n",
      "['thousands', 'of', 'people', 'have', 'been', 'duped', 'by', 'a', 'fake', 'news', 'story', 'claiming', 'that', 'nasa', 'has', 'forecast', 'a', 'total', 'blackout', 'of', 'earth', 'for', 'six', 'days', 'in', 'december', 'the', 'story', 'entitled', 'nasa', 'confirms', 'earth', 'will', 'experience', 'days', 'of', 'total', 'darkness', 'in', 'december', 'originated', 'from', 'huzlers', 'com', 'a', 'website', 'well', 'known', 'for', 'publishing', 'fake', 'stories', 'with', 'sensational', 'headlines', 'the', 'bogus', 'report', 'read', 'nasa', 'has', 'confirmed', 'that', 'the', 'earth', 'will', 'experience', 'days', 'of', 'almost', 'complete', 'darkness', 'and', 'will', 'happen', 'from', 'the', 'dates', 'tuesday', 'the', 'monday', 'the', 'in', 'december', 'the', 'world', 'will', 'remain', 'during', 'these', 'three', 'days', 'without', 'sunlight', 'due', 'to', 'a', 'solar', 'storm', 'which', 'will', 'cause', 'dust', 'and', 'space', 'debris', 'to', 'become', 'plentiful', 'and', 'thus', 'block', 'sunlight', 'the', 'head', 'of', 'nasa', 'charles', 'bolden', 'who', 'made', 'the', 'announcement', 'and', 'asked', 'everyone', 'to', 'remain', 'calm', 'this', 'will', 'be', 'the', 'product', 'of', 'a', 'solar', 'storm', 'the', 'largest', 'in', 'the', 'last', 'years', 'for', 'a', 'period', 'of', 'hours', 'total', 'despite', 'the', 'six', 'days', 'of', 'darkness', 'soon', 'to', 'come', 'officials', 'say', 'that', 'the', 'earth', 'will', 'not', 'experience', 'any', 'major', 'problems', 'since', 'six', 'days', 'of', 'darkness', 'is', 'nowhere', 'near', 'enough', 'to', 'cause', 'major', 'damage', 'to', 'anything', 'adding', 'on', 'the', 'article', 'also', 'carried', 'a', 'made', 'up', 'quote', 'from', 'nasa', 'scientist', 'earl', 'godoy', 'saying', 'we', 'will', 'solely', 'rely', 'on', 'artificial', 'light', 'for', 'the', 'six', 'days', 'which', 'is', 'not', 'a', 'problem', 'at', 'all', 'many', 'twitter', 'users', 'believed', 'the', 'fake', 'news', 'report', 'and', 'expressed', 'their', 'shock', 'we', 're', 'going', 'to', 'have', 'a', 'complete', 'days', 'of', 'darkness', 'due', 'to', 'a', 'solar', 'storm', 'in', 'dec', 'so', 'nervous', 'about', 'this', 'ahhh', 'thepurge', 'http', 't', 'co', 'janella', 'hijanellamarie', 'october', 'days', 'of', 'total', 'darkness', 'in', 'december', 'http', 't', 'co', 'jammie', 'macaranas', 'jammiepeach', 'october', 'nasa', 'confirms', 'earth', 'will', 'experience', 'days', 'of', 'total', 'darkness', 'in', 'december', 'me', 'pic', 'twitter', 'com', 'hiatus', 'tt', 'sarangbces', 'october', 'nasa', 'confirms', 'earth', 'will', 'experience', 'days', 'of', 'total', 'darkness', 'in', 'december', 'omg', 'what', '查理', 'chxrliecutie', 'october', 'islam', 'know', 'what', 'this', 'means', 'im', 'scared', 'nasa', 'confirms', 'earth', 'will', 'experience', 'days', 'of', 'total', 'darkness', 'in', 'december', 'http', 't', 'co', 'gqgeglmelz', 'hiatus', 'taobby', 'october', 'the', 'website', 'has', 'previously', 'published', 'a', 'fake', 'report', 'about', 'american', 'rapper', 'and', 'actor', 'tupac', 'shakur', 'claiming', 'that', 'he', 'is', 'alive', 'relatedhalloween', 'on', 'friday', 'the', 'for', 'first', 'time', 'in', 'years', 'declared', 'a', 'hoaxshah', 'rukh', 'khan', 's', 'son', 'aryan', 'and', 'aishwarya', 'rai', 'bachchan', 's', 'niece', 'navya', 's', 'leaked', 'sex', 'tape', 'is', 'fakeebola', 'zombies', 'victims', 'rising', 'from', 'the', 'dead', 'fake', 'news', 'story', 'goes', 'viral', 'sparks', 'outrage', 'on', 'social', 'mediaeminem', 'quits', 'music', 'after', 'checking', 'into', 'rehab', 'again', 'for', 'heroin', 'addiction', 'is', 'hoax', 'satirical', 'article', 'creates', 'stir', 'on', 'social', 'media']\n",
      "['isis', 'video', 'america', 's', 'air', 'dropped', 'weapons', 'now', 'in', 'our', 'hands']\n",
      "discuss\n"
     ]
    }
   ],
   "source": [
    "print(train_data_articles_df['Article'][154])\n",
    "print(train_data_articles_df['article_cleaned'][154])\n",
    "print(train_data_stances_df['headline_cleaned'][154])\n",
    "print(train_data_stances_df['Stance'][154])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8433fa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "website_words = ['http', 'twitter', 'com', 'pic', 'co']\n",
    "month_words = ['january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', 'novemeber', 'december']\n",
    "\n",
    "my_banned_words = stopwords.words('english') + website_words + month_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d1c666b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(string):\n",
    "    output = []\n",
    "    for word in string:\n",
    "        if word not in my_banned_words:\n",
    "            output.append(word)\n",
    "    return output\n",
    "\n",
    "train_data_articles_df['article_cleaned'] =  train_data_articles_df['article_cleaned'].apply(lambda x: remove_stopwords(x))\n",
    "train_data_stances_df['headline_cleaned'] =  train_data_stances_df['headline_cleaned'].apply(lambda x: remove_stopwords(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c828b2d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thousands', 'people', 'duped', 'fake', 'news', 'story', 'claiming', 'nasa', 'forecast', 'total', 'blackout', 'earth', 'six', 'days', 'story', 'entitled', 'nasa', 'confirms', 'earth', 'experience', 'days', 'total', 'darkness', 'originated', 'huzlers', 'website', 'well', 'known', 'publishing', 'fake', 'stories', 'sensational', 'headlines', 'bogus', 'report', 'read', 'nasa', 'confirmed', 'earth', 'experience', 'days', 'almost', 'complete', 'darkness', 'happen', 'dates', 'tuesday', 'monday', 'world', 'remain', 'three', 'days', 'without', 'sunlight', 'due', 'solar', 'storm', 'cause', 'dust', 'space', 'debris', 'become', 'plentiful', 'thus', 'block', 'sunlight', 'head', 'nasa', 'charles', 'bolden', 'made', 'announcement', 'asked', 'everyone', 'remain', 'calm', 'product', 'solar', 'storm', 'largest', 'last', 'years', 'period', 'hours', 'total', 'despite', 'six', 'days', 'darkness', 'soon', 'come', 'officials', 'say', 'earth', 'experience', 'major', 'problems', 'since', 'six', 'days', 'darkness', 'nowhere', 'near', 'enough', 'cause', 'major', 'damage', 'anything', 'adding', 'article', 'also', 'carried', 'made', 'quote', 'nasa', 'scientist', 'earl', 'godoy', 'saying', 'solely', 'rely', 'artificial', 'light', 'six', 'days', 'problem', 'many', 'users', 'believed', 'fake', 'news', 'report', 'expressed', 'shock', 'going', 'complete', 'days', 'darkness', 'due', 'solar', 'storm', 'dec', 'nervous', 'ahhh', 'thepurge', 'janella', 'hijanellamarie', 'days', 'total', 'darkness', 'jammie', 'macaranas', 'jammiepeach', 'nasa', 'confirms', 'earth', 'experience', 'days', 'total', 'darkness', 'hiatus', 'tt', 'sarangbces', 'nasa', 'confirms', 'earth', 'experience', 'days', 'total', 'darkness', 'omg', '查理', 'chxrliecutie', 'islam', 'know', 'means', 'im', 'scared', 'nasa', 'confirms', 'earth', 'experience', 'days', 'total', 'darkness', 'gqgeglmelz', 'hiatus', 'taobby', 'website', 'previously', 'published', 'fake', 'report', 'american', 'rapper', 'actor', 'tupac', 'shakur', 'claiming', 'alive', 'relatedhalloween', 'friday', 'first', 'time', 'years', 'declared', 'hoaxshah', 'rukh', 'khan', 'son', 'aryan', 'aishwarya', 'rai', 'bachchan', 'niece', 'navya', 'leaked', 'sex', 'tape', 'fakeebola', 'zombies', 'victims', 'rising', 'dead', 'fake', 'news', 'story', 'goes', 'viral', 'sparks', 'outrage', 'social', 'mediaeminem', 'quits', 'music', 'checking', 'rehab', 'heroin', 'addiction', 'hoax', 'satirical', 'article', 'creates', 'stir', 'social', 'media']\n"
     ]
    }
   ],
   "source": [
    "print(train_data_articles_df['article_cleaned'][154])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1b80325",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('processed'):\n",
    "    os.mkdir('processed')\n",
    "\n",
    "train_data_articles_df.to_csv(\"processed/processed_train_articles.csv\")\n",
    "train_data_stances_df.to_csv(\"processed/processed_train_stances.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f7976c",
   "metadata": {},
   "source": [
    "\n",
    "concatenate headline with article, extract features using bag of words, tfidf score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6998a874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Headline  Body ID     Stance  \\\n",
      "0  Police find mass graves with at least '15 bodi...      712  unrelated   \n",
      "1  Hundreds of Palestinians flee floods in Gaza a...      158      agree   \n",
      "2  Christian Bale passes on role of Steve Jobs, a...      137  unrelated   \n",
      "3  HBO and Apple in Talks for $15/Month Apple TV ...     1034  unrelated   \n",
      "4  Spider burrowed through tourist's stomach and ...     1923   disagree   \n",
      "\n",
      "                                    headline_cleaned  \n",
      "0  [police, find, mass, graves, least, bodies, ne...  \n",
      "1  [hundreds, palestinians, flee, floods, gaza, i...  \n",
      "2  [christian, bale, passes, role, steve, jobs, a...  \n",
      "3  [hbo, apple, talks, month, apple, tv, streamin...  \n",
      "4        [spider, burrowed, tourist, stomach, chest]  \n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'describes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(train_data_stances_df\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtrain_data_stances_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBody ID\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdescribes\u001b[49m())\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(train_data_articles_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBody ID\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts())\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu-1\\lib\\site-packages\\pandas\\core\\generic.py:5575\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   5569\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[0;32m   5570\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[0;32m   5571\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[0;32m   5572\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[0;32m   5573\u001b[0m ):\n\u001b[0;32m   5574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[1;32m-> 5575\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Series' object has no attribute 'describes'"
     ]
    }
   ],
   "source": [
    "print(train_data_stances_df.head(5))\n",
    "print(train_data_stances_df['Body ID'].value_counts())\n",
    "print(train_data_articles_df['Body ID'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc6dae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for index, article in train_data_articles_df['Article'].items():\n",
    "index = 0\n",
    "body_id = train_data_articles_df['Body ID'].iloc(index)\n",
    "headline = train_data_stances_df.loc(train_data_stances_df['Body ID'] == body_id, 'Headline');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d40acef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "vectors = vectorizer.fit_transform()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94a75870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(246, 170)\n"
     ]
    }
   ],
   "source": [
    "print(train_data_articles_df['article_vector'][154].)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5634b676",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu-1",
   "language": "python",
   "name": "gpu-1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
