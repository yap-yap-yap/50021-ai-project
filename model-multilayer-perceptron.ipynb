{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc4c0875",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer, regexp_tokenize, word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import sys\n",
    "import os\n",
    "import string\n",
    "import re\n",
    "from tqdm import tqdm \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "from utils import dataset, generate_test_splits\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2476ccb6",
   "metadata": {},
   "source": [
    "### Using the included data importer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dc6589",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99c5090b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset\n",
      "Total stances: 49972\n",
      "Total bodies: 1683\n"
     ]
    }
   ],
   "source": [
    "train_data = dataset.DataSet(name='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c88b88",
   "metadata": {},
   "source": [
    "### Converting imported data into pandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2497417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hundreds of Palestinians were evacuated from their homes Sunday morning after Israeli authorities opened a number of dams near the border, flooding the Gaza Valley in the wake of a recent severe winter storm.\n",
      "\n",
      "The Gaza Ministry of Interior said in a statement that civil defense services and teams from the Ministry of Public Works had evacuated more than 80 families from both sides of the Gaza Valley (Wadi Gaza) after their homes flooded as water levels reached more than three meters.\n",
      "\n",
      "Gaza has experienced flooding in recent days amid a major storm that saw temperatures drop and frigid rain pour down.\n",
      "\n",
      "The storm displaced dozens and caused hardship for tens of thousands, including many of the approximately 110,000 Palestinians left homeless by Israel's assault over summer.\n",
      "\n",
      "The suffering is compounded by the fact that Israel has maintained a complete siege over Gaza for the last eight years, severely limiting electricity and the availability of fuel for generators. It has also prevented the displaced from rebuilding their homes, as construction materials are largely banned from entering.\n",
      "\n",
      "Gaza civil defense services spokesman Muhammad al-Midana warned that further harm could be caused if Israel opens up more dams in the area, noting that water is currently flowing at a high speed from the Israel border through the valley and into the Mediterranean sea.\n",
      "\n",
      "Evacuated families have been sent to shelters sponsored by UNRWA, the UN agency for Palestinian refugees, in al-Bureij refugee camp and in al-Zahra neighborhood in the central Gaza Strip.\n",
      "\n",
      "The Gaza Valley (Wadi Gaza) is a wetland located in the central Gaza Strip between al-Nuseirat refugee camp and al-Moghraqa. It is called HaBesor in Hebrew, and it flows from two streams -- one whose source runs from near Beersheba, and the other from near Hebron.\n",
      "\n",
      "Israeli dams on the river to collect rainwater have dried up the wetlands inside Gaza, and destroyed the only source of surface water in the area.\n",
      "\n",
      "Locals have continued to use it to dispose of their waste for lack of other ways to do so, however, creating an environmental hazard.\n",
      "\n",
      "This is not the first time Israeli authorities have opened the Gaza Valley dams.\n",
      "\n",
      "In Dec. 2013, Israeli authorities also opened the dams amid heavy flooding in the Gaza Strip. The resulting floods damaged dozens of homes and forces many families in the area from their homes.\n",
      "\n",
      "In 2010, the dams were opened as well, forcing 100 families from their homes. At the time civil defense services said that they had managed to save seven people who had been at risk of drowning.\n",
      "\n",
      "'Hairgate': iPhone 6 users encounter latest Apple problem after 'bendgate'\n",
      "\n",
      "unrelated\n"
     ]
    }
   ],
   "source": [
    "train_data_articles_df = pd.DataFrame({'Article': train_data.articles.values(), 'Body ID':train_data.articles.keys()} , index=train_data.articles.keys())\n",
    "train_data_stances_df = pd.DataFrame(train_data.stances)\n",
    "\n",
    "# checking out example output\n",
    "print(train_data_articles_df['Article'][158] + '\\n')\n",
    "print(train_data_stances_df['Headline'][158] + '\\n')\n",
    "print(train_data_stances_df['Stance'][158])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d38444",
   "metadata": {},
   "source": [
    "### Preprocessing: Tokenize and remove stopwords. \n",
    "\n",
    "Can do more, obvious ones: remove news agency header, remove website URL, remove twitter usernames, stemming, lemmatizating \n",
    "\n",
    "More done later during vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae78dca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\b[^\\d\\W]+\\b') # regex can be improved but idk\n",
    "train_data_articles_df['article_cleaned'] = train_data_articles_df['Article'].apply(lambda x: tokenizer.tokenize(x.lower()))\n",
    "train_data_stances_df['headline_cleaned'] = train_data_stances_df['Headline'].apply(lambda x: tokenizer.tokenize(x.lower()))\n",
    "\n",
    "# remove_whitespace = r'\\s+'\n",
    "# train_data_articles_df['article_cleaned'] = train_data_articles_df['Article'].apply(lambda x: re.split(remove_whitespace, x))\n",
    "# train_data_stances_df['headline_cleaned'] = train_data_stances_df['Headline'].apply(lambda x: re.split(remove_whitespace, x))\n",
    "\n",
    "# exclude = r'[^/d/W]+'\n",
    "# train_data_articles_df['article_cleaned'] = train_data_articles_df['article_cleaned'].apply(lambda x: re.findall(exclude, x))\n",
    "# train_data_stances_df['headline_cleaned'] = train_data_stances_df['headline_cleaned'].apply(lambda x: re.findall(exclude, x))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d3f605e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thousands of people have been duped by a fake news story claiming that Nasa has forecast a total blackout of earth for six days in December.\n",
      "\n",
      "The story, entitled \"Nasa Confirms Earth Will Experience 6 Days of Total Darkness in December 2014!\" originated from Huzlers.com, a website well known for publishing fake stories with sensational headlines.\n",
      "\n",
      "The bogus report read: \"Nasa has confirmed that the Earth will experience 6 days of almost complete darkness and will happen from the dates Tuesday the 16 – Monday the 22 in December. The world will remain, during these three days, without sunlight due to a solar storm, which will cause dust and space debris to become plentiful and thus, block 90% sunlight.\n",
      "\n",
      "\"The head of Nasa Charles Bolden who made the announcement and asked everyone to remain calm. This will be the product of a solar storm, the largest in the last 250 years for a period of 216 hours total.\n",
      "\n",
      "\"Despite the six days of darkness soon to come, officials say that the earth will not experience any major problems, since six days of darkness is nowhere near enough to cause major damage to anything.\"\n",
      "\n",
      "Adding on, the article also carried a made-up quote from Nasa scientist Earl Godoy, saying: \"We will solely rely on artificial light for the six days, which is not a problem at all.\"\n",
      "\n",
      "Many Twitter users believed the fake news report, and expressed their shock.\n",
      "\n",
      "We're going to have a complete 6 days of darkness due to a solar storm in Dec! SO NERVOUS ABOUT THIS! Ahhh. #ThePurge http://t.co/0L2Sis54hv— Janella (@hijanellamarie) October 26, 2014 6 days of total darkness in December? ? http://t.co/eTN60TnXft— Jammie Macaranas (@JammiePeach) October 26, 2014 \"NASA Confirms Earth will experience 6 Days of total DARKNESS in December 2014.\" Me: pic.twitter.com/xZG1xaxqdw— [Hiatus] TT (@sarangBCES) October 26, 2014 \"NASA Confirms Earth Will Experience 6 Days of Total Darkness in December 2014!\" omg what?— 查理 (@Chxrliecutie) October 26, 2014 islam know what this means im scared \"NASA Confirms Earth Will Experience 6 Days of Total Darkness in December 2014! http://t.co/GQGeGLmElZ\"— hiatus (@taobby) October 26, 2014\n",
      "\n",
      "The website has previously published a fake report about American rapper and actor Tupac Shakur, claiming that he is alive.\n",
      "\n",
      "RelatedHalloween 2014 on Friday the 13th for First Time in 666 Years Declared a HoaxShah Rukh Khan's Son Aryan and Aishwarya Rai Bachchan's Niece Navya's Leaked Sex Tape is FakeEbola Zombies: Victims 'Rising from the Dead' Fake News Story Goes Viral, Sparks Outrage on Social MediaEminem 'Quits Music After Checking Into Rehab Again For Heroin Addiction' is Hoax: Satirical Article Creates Stir on Social Media\n",
      "['thousands', 'of', 'people', 'have', 'been', 'duped', 'by', 'a', 'fake', 'news', 'story', 'claiming', 'that', 'nasa', 'has', 'forecast', 'a', 'total', 'blackout', 'of', 'earth', 'for', 'six', 'days', 'in', 'december', 'the', 'story', 'entitled', 'nasa', 'confirms', 'earth', 'will', 'experience', 'days', 'of', 'total', 'darkness', 'in', 'december', 'originated', 'from', 'huzlers', 'com', 'a', 'website', 'well', 'known', 'for', 'publishing', 'fake', 'stories', 'with', 'sensational', 'headlines', 'the', 'bogus', 'report', 'read', 'nasa', 'has', 'confirmed', 'that', 'the', 'earth', 'will', 'experience', 'days', 'of', 'almost', 'complete', 'darkness', 'and', 'will', 'happen', 'from', 'the', 'dates', 'tuesday', 'the', 'monday', 'the', 'in', 'december', 'the', 'world', 'will', 'remain', 'during', 'these', 'three', 'days', 'without', 'sunlight', 'due', 'to', 'a', 'solar', 'storm', 'which', 'will', 'cause', 'dust', 'and', 'space', 'debris', 'to', 'become', 'plentiful', 'and', 'thus', 'block', 'sunlight', 'the', 'head', 'of', 'nasa', 'charles', 'bolden', 'who', 'made', 'the', 'announcement', 'and', 'asked', 'everyone', 'to', 'remain', 'calm', 'this', 'will', 'be', 'the', 'product', 'of', 'a', 'solar', 'storm', 'the', 'largest', 'in', 'the', 'last', 'years', 'for', 'a', 'period', 'of', 'hours', 'total', 'despite', 'the', 'six', 'days', 'of', 'darkness', 'soon', 'to', 'come', 'officials', 'say', 'that', 'the', 'earth', 'will', 'not', 'experience', 'any', 'major', 'problems', 'since', 'six', 'days', 'of', 'darkness', 'is', 'nowhere', 'near', 'enough', 'to', 'cause', 'major', 'damage', 'to', 'anything', 'adding', 'on', 'the', 'article', 'also', 'carried', 'a', 'made', 'up', 'quote', 'from', 'nasa', 'scientist', 'earl', 'godoy', 'saying', 'we', 'will', 'solely', 'rely', 'on', 'artificial', 'light', 'for', 'the', 'six', 'days', 'which', 'is', 'not', 'a', 'problem', 'at', 'all', 'many', 'twitter', 'users', 'believed', 'the', 'fake', 'news', 'report', 'and', 'expressed', 'their', 'shock', 'we', 're', 'going', 'to', 'have', 'a', 'complete', 'days', 'of', 'darkness', 'due', 'to', 'a', 'solar', 'storm', 'in', 'dec', 'so', 'nervous', 'about', 'this', 'ahhh', 'thepurge', 'http', 't', 'co', 'janella', 'hijanellamarie', 'october', 'days', 'of', 'total', 'darkness', 'in', 'december', 'http', 't', 'co', 'jammie', 'macaranas', 'jammiepeach', 'october', 'nasa', 'confirms', 'earth', 'will', 'experience', 'days', 'of', 'total', 'darkness', 'in', 'december', 'me', 'pic', 'twitter', 'com', 'hiatus', 'tt', 'sarangbces', 'october', 'nasa', 'confirms', 'earth', 'will', 'experience', 'days', 'of', 'total', 'darkness', 'in', 'december', 'omg', 'what', '查理', 'chxrliecutie', 'october', 'islam', 'know', 'what', 'this', 'means', 'im', 'scared', 'nasa', 'confirms', 'earth', 'will', 'experience', 'days', 'of', 'total', 'darkness', 'in', 'december', 'http', 't', 'co', 'gqgeglmelz', 'hiatus', 'taobby', 'october', 'the', 'website', 'has', 'previously', 'published', 'a', 'fake', 'report', 'about', 'american', 'rapper', 'and', 'actor', 'tupac', 'shakur', 'claiming', 'that', 'he', 'is', 'alive', 'relatedhalloween', 'on', 'friday', 'the', 'for', 'first', 'time', 'in', 'years', 'declared', 'a', 'hoaxshah', 'rukh', 'khan', 's', 'son', 'aryan', 'and', 'aishwarya', 'rai', 'bachchan', 's', 'niece', 'navya', 's', 'leaked', 'sex', 'tape', 'is', 'fakeebola', 'zombies', 'victims', 'rising', 'from', 'the', 'dead', 'fake', 'news', 'story', 'goes', 'viral', 'sparks', 'outrage', 'on', 'social', 'mediaeminem', 'quits', 'music', 'after', 'checking', 'into', 'rehab', 'again', 'for', 'heroin', 'addiction', 'is', 'hoax', 'satirical', 'article', 'creates', 'stir', 'on', 'social', 'media']\n",
      "['isis', 'video', 'america', 's', 'air', 'dropped', 'weapons', 'now', 'in', 'our', 'hands']\n",
      "discuss\n"
     ]
    }
   ],
   "source": [
    "print(train_data_articles_df['Article'][154])\n",
    "print(train_data_articles_df['article_cleaned'][154])\n",
    "print(train_data_stances_df['headline_cleaned'][154])\n",
    "print(train_data_stances_df['Stance'][154])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8433fa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "website_words = ['http', 'twitter', 'com', 'pic', 'co']\n",
    "month_words = ['january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', 'novemeber', 'december']\n",
    "\n",
    "my_banned_words = stopwords.words('english') + website_words + month_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d1c666b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(string):\n",
    "    output = []\n",
    "    for word in string:\n",
    "        if word not in my_banned_words:\n",
    "            output.append(word)\n",
    "    return output\n",
    "\n",
    "train_data_articles_df['article_cleaned'] =  train_data_articles_df['article_cleaned'].apply(lambda x: remove_stopwords(x))\n",
    "train_data_stances_df['headline_cleaned'] =  train_data_stances_df['headline_cleaned'].apply(lambda x: remove_stopwords(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c828b2d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thousands', 'people', 'duped', 'fake', 'news', 'story', 'claiming', 'nasa', 'forecast', 'total', 'blackout', 'earth', 'six', 'days', 'story', 'entitled', 'nasa', 'confirms', 'earth', 'experience', 'days', 'total', 'darkness', 'originated', 'huzlers', 'website', 'well', 'known', 'publishing', 'fake', 'stories', 'sensational', 'headlines', 'bogus', 'report', 'read', 'nasa', 'confirmed', 'earth', 'experience', 'days', 'almost', 'complete', 'darkness', 'happen', 'dates', 'tuesday', 'monday', 'world', 'remain', 'three', 'days', 'without', 'sunlight', 'due', 'solar', 'storm', 'cause', 'dust', 'space', 'debris', 'become', 'plentiful', 'thus', 'block', 'sunlight', 'head', 'nasa', 'charles', 'bolden', 'made', 'announcement', 'asked', 'everyone', 'remain', 'calm', 'product', 'solar', 'storm', 'largest', 'last', 'years', 'period', 'hours', 'total', 'despite', 'six', 'days', 'darkness', 'soon', 'come', 'officials', 'say', 'earth', 'experience', 'major', 'problems', 'since', 'six', 'days', 'darkness', 'nowhere', 'near', 'enough', 'cause', 'major', 'damage', 'anything', 'adding', 'article', 'also', 'carried', 'made', 'quote', 'nasa', 'scientist', 'earl', 'godoy', 'saying', 'solely', 'rely', 'artificial', 'light', 'six', 'days', 'problem', 'many', 'users', 'believed', 'fake', 'news', 'report', 'expressed', 'shock', 'going', 'complete', 'days', 'darkness', 'due', 'solar', 'storm', 'dec', 'nervous', 'ahhh', 'thepurge', 'janella', 'hijanellamarie', 'days', 'total', 'darkness', 'jammie', 'macaranas', 'jammiepeach', 'nasa', 'confirms', 'earth', 'experience', 'days', 'total', 'darkness', 'hiatus', 'tt', 'sarangbces', 'nasa', 'confirms', 'earth', 'experience', 'days', 'total', 'darkness', 'omg', '查理', 'chxrliecutie', 'islam', 'know', 'means', 'im', 'scared', 'nasa', 'confirms', 'earth', 'experience', 'days', 'total', 'darkness', 'gqgeglmelz', 'hiatus', 'taobby', 'website', 'previously', 'published', 'fake', 'report', 'american', 'rapper', 'actor', 'tupac', 'shakur', 'claiming', 'alive', 'relatedhalloween', 'friday', 'first', 'time', 'years', 'declared', 'hoaxshah', 'rukh', 'khan', 'son', 'aryan', 'aishwarya', 'rai', 'bachchan', 'niece', 'navya', 'leaked', 'sex', 'tape', 'fakeebola', 'zombies', 'victims', 'rising', 'dead', 'fake', 'news', 'story', 'goes', 'viral', 'sparks', 'outrage', 'social', 'mediaeminem', 'quits', 'music', 'checking', 'rehab', 'heroin', 'addiction', 'hoax', 'satirical', 'article', 'creates', 'stir', 'social', 'media']\n"
     ]
    }
   ],
   "source": [
    "print(train_data_articles_df['article_cleaned'][154])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1b80325",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('processed'):\n",
    "    os.mkdir('processed')\n",
    "\n",
    "train_data_articles_df.to_csv(\"processed/processed_train_articles.csv\")\n",
    "train_data_stances_df.to_csv(\"processed/processed_train_stances.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d054dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0  1787  3974  4936  5210  5863  6199  6756  7526  9003 10036 10780\n",
      " 11687 11864 15746 21620 21712 21928 22100 25006 25492 25616 26260 26398\n",
      " 27200 29988 33683 37095 38326 41035 42776 43897 44978 45222 45579 46530\n",
      " 47712 47850 48228]\n"
     ]
    }
   ],
   "source": [
    "stance_df = train_data_stances_df[train_data_stances_df['Body ID'] == 712]\n",
    "print(stance_df.index.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f7976c",
   "metadata": {},
   "source": [
    "\n",
    " ### Strategy: concatenate headline with article, encode as bag of words, tfidf score\n",
    " \n",
    "TODO: more to try: only tf, tf of headline and article + tfidf cosine similarity (ranked 3rd group's strat), process headline and article separately (can then run an algorithm separately on each part, then concat and run fully connected) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8be437a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  4  5  6  7  8  9 10 11 13]\n",
      "[2102 1120 2414   75  298  720 1340  931 2448 2032]\n"
     ]
    }
   ],
   "source": [
    "stance_to_number = {\n",
    "    \"agree\": 0,\n",
    "    \"disagree\": 1,\n",
    "    \"discuss\": 2,\n",
    "    \"unrelated\": 3\n",
    "}\n",
    "\n",
    "        \n",
    "def concat_data(articles_df, headlines_df, body_ids):\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    for body_id in body_ids:\n",
    "        article = articles_df[articles_df['Body ID'] == body_id]['article_cleaned'].values[0]\n",
    "    #     print(article)\n",
    "        headlines = headlines_df[headlines_df['Body ID'] == body_id]['headline_cleaned'].values\n",
    "    #     print(headlines)\n",
    "        stances = headlines_df[headlines_df['Body ID'] == body_id]['Stance'].values\n",
    "    #     print(stances)\n",
    "        for headline, stance in zip(headlines, stances):\n",
    "            features.append(headline+article)\n",
    "#             label_idx = stance_to_number[stance]\n",
    "#             label = [0.0, 0.0, 0.0, 0.0] # one hot encoding for labels\n",
    "#             label[label_idx] = 1.0\n",
    "#             labels.append(label)\n",
    "            labels.append(stance_to_number[stance])\n",
    "    return features, labels\n",
    "\n",
    "body_ids = train_data_articles_df['Body ID'].copy().values\n",
    "print(body_ids[0:10])\n",
    "np.random.seed(42) # set your seed\n",
    "np.random.shuffle(body_ids) # randomise it here and then do the train/val split later directly. If randomised later then validation and training set each will draw from their own datasets\n",
    "print(body_ids[0:10])\n",
    "features, labels = concat_data(train_data_articles_df, train_data_stances_df, body_ids) # article, headline and stances are retrieved at the same time so splitting later is fine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7763744f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['unconfirmed', 'reports', 'saying', 'isis', 'uk', 'hostage', 'david', 'haines', 'beheaded', 'uk', 'confirmed', 'video', 'showing', 'beheading', 'aid', 'worker', 'david', 'haines']\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(features[0])\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f65430d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49972, 5000)\n"
     ]
    }
   ],
   "source": [
    "# dummy preprocessor and tokenizer because already did that above. not going to change it up to fit this format\n",
    "num_features = 5000\n",
    "\n",
    "def dummy(doc):\n",
    "    return doc\n",
    "vectorizer = CountVectorizer(preprocessor=dummy, tokenizer=dummy, max_features=num_features) # try messing with min_df param: can remove twitter handles and such\n",
    "features_counts = vectorizer.fit_transform(features)\n",
    "\n",
    "\n",
    "print(features_counts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f388ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aamir' 'aaron' 'abandoned' 'abandoning' 'abc' 'abdel' 'abdi' 'abdomen'\n",
      " 'abducted' 'abducting' 'abduction' 'abductions' 'abdul' 'abdulla'\n",
      " 'abdullah' 'ability' 'able' 'abortion' 'abrams' 'abroad' 'absence'\n",
      " 'absent' 'absenteeism' 'absolutely' 'abu' 'abubakar' 'abuja' 'abuse'\n",
      " 'abuses' 'academy' 'accent' 'accept' 'accepted' 'access' 'accessories'\n",
      " 'accident' 'accidental' 'accidentally' 'accompanied' 'according'\n",
      " 'account' 'accounts' 'accurate' 'accusations' 'accused' 'achieve'\n",
      " 'acknowledged' 'acquisition' 'acre' 'across']\n",
      "(array([], dtype=int64),)\n"
     ]
    }
   ],
   "source": [
    "# more preprocessing: dropping very common non-stopword words and very uncommon words\n",
    "# features_counts_flip = np.flip(features_counts)\n",
    "# features_counts_df = pd.DataFrame(features_counts_flip, columns=vectorizer.get_feature_names_out())\n",
    "# print(features_counts_df.head)\n",
    "\n",
    "print(vectorizer.get_feature_names_out()[0:50])\n",
    "print(np.where(vectorizer.get_feature_names_out() == \"hijanellamarie\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8190dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOW do the train val split. \n",
    "# Vectorise and get bag of words for both train + test first so that the features are the same length\n",
    "# can also instead generate a dictionary beforehand to pass into the vectorizer\n",
    "\n",
    "train_percentage = 0.8\n",
    "num_samples = features_counts.shape[0]\n",
    "split_index = int(train_percentage*num_samples)\n",
    "train_features_counts = features_counts[:split_index]\n",
    "val_features_counts = features_counts[split_index:]\n",
    "train_labels = labels[:split_index]\n",
    "val_labels = labels[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5634b676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39977, 5000)\n"
     ]
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "train_features_tfidf = tfidf_transformer.fit_transform(train_features_counts)\n",
    "val_features_tfidf = tfidf_transformer.fit_transform(val_features_counts)\n",
    "\n",
    "print(train_features_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f4f8dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the vectors\n",
    "# train_features_tfidf_nparray = train_features_tfidf.toarray()\n",
    "# test_features_tfidf_nparray = test_features_tfidf.toarray()\n",
    "\n",
    "# np.savetxt(\"processed/vector_train_features_tdidf.txt\", train_features_tfidf_nparray)\n",
    "# np.savetxt(\"processed/vector_test_features_tdidf.txt\", test_features_tfidf_nparray)\n",
    "# >22GB\n",
    "\n",
    "# the file size is so big for the vectorized array \n",
    "# i think it's better to save the data as just list of sentences and load them, \n",
    "# then do the vectorization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd308499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5000)\n"
     ]
    }
   ],
   "source": [
    "print(train_features_tfidf[0].toarray().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df21bf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features=in_dim, out_features=2500)\n",
    "        self.fc2 = nn.Linear(in_features=2500, out_features=1000)\n",
    "        self.fc3 = nn.Linear(in_features=1000, out_features=100)\n",
    "        self.fc4 = nn.Linear(in_features=100, out_features=out_dim)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bfd392e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dimensions = num_features\n",
    "out_dimensions = 4\n",
    "\n",
    "model = MultiLayerPerceptron(input_dimensions, out_dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e85a94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b2e75fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "18a555b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250\n"
     ]
    }
   ],
   "source": [
    "# for batches \n",
    "batch_size = 32\n",
    "\n",
    "train_features_tfidf_array = train_features_tfidf.toarray()\n",
    "val_features_tfidf_array = val_features_tfidf.toarray()\n",
    "\n",
    "train_iterator = data.DataLoader([[train_features_tfidf_array[i], train_labels[i]] for i in range(len(train_labels))],\n",
    "                                 shuffle=True,\n",
    "                                 batch_size=batch_size,\n",
    "                                 num_workers=4)\n",
    "\n",
    "val_iterator = data.DataLoader([[val_features_tfidf_array[i], val_labels[i]] for i in range(len(val_labels))],\n",
    "                                 shuffle=True,\n",
    "                                 batch_size=batch_size,\n",
    "                                 num_workers=4)\n",
    "\n",
    "train_iterator_size = len(train_iterator)\n",
    "val_iterator_size = len(val_iterator)\n",
    "print(train_iterator_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4e8dd549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y, label):\n",
    "#     y.view(1, 4)\n",
    "#     label.view(1,)\n",
    "#     print(y)\n",
    "#     print(label)\n",
    "    top_pred = y.argmax(1, keepdim=True)\n",
    "    correct = top_pred.eq(label.view_as(top_pred)).sum()\n",
    "    acc = correct.float() / label.shape[0]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d9cb4d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 training: 100%|████████████████████████████████████████████████████████████| 1250/1250 [00:38<00:00, 32.72it/s]\n",
      "Epoch 1 validating: 100%|████████████████████████████████████████████████████████████| 313/313 [00:05<00:00, 54.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss = 0.346; train acc = 86.976%\n",
      "val loss = 0.849; val acc = 69.701%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 training: 100%|████████████████████████████████████████████████████████████| 1250/1250 [00:39<00:00, 31.60it/s]\n",
      "Epoch 2 validating: 100%|████████████████████████████████████████████████████████████| 313/313 [00:05<00:00, 53.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss = 0.280; train acc = 89.333%\n",
      "val loss = 0.932; val acc = 67.933%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 training: 100%|████████████████████████████████████████████████████████████| 1250/1250 [00:33<00:00, 36.83it/s]\n",
      "Epoch 3 validating: 100%|████████████████████████████████████████████████████████████| 313/313 [00:05<00:00, 53.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss = 0.239; train acc = 90.554%\n",
      "val loss = 0.997; val acc = 68.992%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 training: 100%|████████████████████████████████████████████████████████████| 1250/1250 [00:33<00:00, 37.54it/s]\n",
      "Epoch 4 validating: 100%|████████████████████████████████████████████████████████████| 313/313 [00:05<00:00, 58.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss = 0.207; train acc = 91.630%\n",
      "val loss = 1.150; val acc = 65.648%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 training: 100%|████████████████████████████████████████████████████████████| 1250/1250 [00:35<00:00, 34.72it/s]\n",
      "Epoch 5 validating: 100%|████████████████████████████████████████████████████████████| 313/313 [00:05<00:00, 54.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss = 0.186; train acc = 92.553%\n",
      "val loss = 1.140; val acc = 68.005%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    for x, label in tqdm(train_iterator, desc=f\"Epoch {epoch+1} training\"):\n",
    "#     for idx in tqdm(range(train_features_tfidf.shape[0]), desc=f\"Epoch {epoch+1} training\"):\n",
    "#         x = train_features_tensor[idx].float()\n",
    "#         label = train_labels_tensor[idx].long()\n",
    "        \n",
    "        x = x.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        x = x.float() \n",
    "        label = label.long()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        y = model(x)\n",
    "\n",
    "        loss = criterion(y, label)\n",
    "        acc = calculate_accuracy(y, label)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        train_acc += acc.item()\n",
    "        \n",
    "    train_loss /= train_iterator_size\n",
    "    train_acc /= train_iterator_size\n",
    "    \n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    with torch.no_grad():\n",
    "        for x, label in tqdm(val_iterator, desc=f\"Epoch {epoch+1} validating\"):\n",
    "#         for idx in tqdm(range(val_features_tfidf.shape[0]), desc=f\"Epoch {epoch+1} validating\"):\n",
    "#             x = val_features_tensor[idx].float()\n",
    "#             label = val_labels_tensor[idx].long()\n",
    "            x = x.to(device)\n",
    "            label = label.to(device)\n",
    "            \n",
    "            x = x.float() \n",
    "            label = label.long()\n",
    "\n",
    "            y = model(x)\n",
    "\n",
    "            loss = criterion(y, label)\n",
    "            acc = calculate_accuracy(y, label)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            val_acc += acc.item()\n",
    "            \n",
    "    val_loss /= val_iterator_size\n",
    "    val_acc /= val_iterator_size\n",
    "\n",
    "    print(f\"train loss = {train_loss:.3f}; train acc = {train_acc*100:.3f}%\")\n",
    "    print(f\"val loss = {val_loss:.3f}; val acc = {val_acc*100:.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab026933",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81063965",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu-1",
   "language": "python",
   "name": "gpu-1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
