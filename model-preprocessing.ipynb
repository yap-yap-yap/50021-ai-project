{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dc4c0875",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer, regexp_tokenize, word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import sys\n",
    "import os\n",
    "import string\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from utils import dataset, generate_test_splits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2476ccb6",
   "metadata": {},
   "source": [
    "### Using the included data importer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67dc6589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99c5090b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset\n",
      "Total stances: 49972\n",
      "Total bodies: 1683\n"
     ]
    }
   ],
   "source": [
    "train_data = dataset.DataSet(name='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ade9858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split test and train splits\n",
    "generate_test_splits.generate_hold_out_split(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c88b88",
   "metadata": {},
   "source": [
    "### Converting imported data into pandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2497417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hundreds of Palestinians were evacuated from their homes Sunday morning after Israeli authorities opened a number of dams near the border, flooding the Gaza Valley in the wake of a recent severe winter storm.\n",
      "\n",
      "The Gaza Ministry of Interior said in a statement that civil defense services and teams from the Ministry of Public Works had evacuated more than 80 families from both sides of the Gaza Valley (Wadi Gaza) after their homes flooded as water levels reached more than three meters.\n",
      "\n",
      "Gaza has experienced flooding in recent days amid a major storm that saw temperatures drop and frigid rain pour down.\n",
      "\n",
      "The storm displaced dozens and caused hardship for tens of thousands, including many of the approximately 110,000 Palestinians left homeless by Israel's assault over summer.\n",
      "\n",
      "The suffering is compounded by the fact that Israel has maintained a complete siege over Gaza for the last eight years, severely limiting electricity and the availability of fuel for generators. It has also prevented the displaced from rebuilding their homes, as construction materials are largely banned from entering.\n",
      "\n",
      "Gaza civil defense services spokesman Muhammad al-Midana warned that further harm could be caused if Israel opens up more dams in the area, noting that water is currently flowing at a high speed from the Israel border through the valley and into the Mediterranean sea.\n",
      "\n",
      "Evacuated families have been sent to shelters sponsored by UNRWA, the UN agency for Palestinian refugees, in al-Bureij refugee camp and in al-Zahra neighborhood in the central Gaza Strip.\n",
      "\n",
      "The Gaza Valley (Wadi Gaza) is a wetland located in the central Gaza Strip between al-Nuseirat refugee camp and al-Moghraqa. It is called HaBesor in Hebrew, and it flows from two streams -- one whose source runs from near Beersheba, and the other from near Hebron.\n",
      "\n",
      "Israeli dams on the river to collect rainwater have dried up the wetlands inside Gaza, and destroyed the only source of surface water in the area.\n",
      "\n",
      "Locals have continued to use it to dispose of their waste for lack of other ways to do so, however, creating an environmental hazard.\n",
      "\n",
      "This is not the first time Israeli authorities have opened the Gaza Valley dams.\n",
      "\n",
      "In Dec. 2013, Israeli authorities also opened the dams amid heavy flooding in the Gaza Strip. The resulting floods damaged dozens of homes and forces many families in the area from their homes.\n",
      "\n",
      "In 2010, the dams were opened as well, forcing 100 families from their homes. At the time civil defense services said that they had managed to save seven people who had been at risk of drowning.\n",
      "\n",
      "'Hairgate': iPhone 6 users encounter latest Apple problem after 'bendgate'\n",
      "\n",
      "unrelated\n"
     ]
    }
   ],
   "source": [
    "train_data_articles_df = pd.DataFrame({'Article': train_data.articles.values(), 'Body ID':train_data.articles.keys()} , index=train_data.articles.keys())\n",
    "train_data_stances_df = pd.DataFrame(train_data.stances)\n",
    "\n",
    "# checking out example output\n",
    "print(train_data_articles_df['Article'][158] + '\\n')\n",
    "print(train_data_stances_df['Headline'][158] + '\\n')\n",
    "print(train_data_stances_df['Stance'][158])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d38444",
   "metadata": {},
   "source": [
    "### Preprocessing: Tokenize and remove stopwords. \n",
    "\n",
    "Can do more, obvious ones: remove news agency header, remove website URL, remove twitter usernames, stemming, lemmatizating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae78dca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\b[^\\d\\W]+\\b') # regex can be improved but idk\n",
    "train_data_articles_df['article_cleaned'] = train_data_articles_df['Article'].apply(lambda x: tokenizer.tokenize(x.lower()))\n",
    "train_data_stances_df['headline_cleaned'] = train_data_stances_df['Headline'].apply(lambda x: tokenizer.tokenize(x.lower()))\n",
    "\n",
    "# remove_whitespace = r'\\s+'\n",
    "# train_data_articles_df['article_cleaned'] = train_data_articles_df['Article'].apply(lambda x: re.split(remove_whitespace, x))\n",
    "# train_data_stances_df['headline_cleaned'] = train_data_stances_df['Headline'].apply(lambda x: re.split(remove_whitespace, x))\n",
    "\n",
    "# exclude = r'[^/d/W]+'\n",
    "# train_data_articles_df['article_cleaned'] = train_data_articles_df['article_cleaned'].apply(lambda x: re.findall(exclude, x))\n",
    "# train_data_stances_df['headline_cleaned'] = train_data_stances_df['headline_cleaned'].apply(lambda x: re.findall(exclude, x))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d3f605e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thousands of people have been duped by a fake news story claiming that Nasa has forecast a total blackout of earth for six days in December.\n",
      "\n",
      "The story, entitled \"Nasa Confirms Earth Will Experience 6 Days of Total Darkness in December 2014!\" originated from Huzlers.com, a website well known for publishing fake stories with sensational headlines.\n",
      "\n",
      "The bogus report read: \"Nasa has confirmed that the Earth will experience 6 days of almost complete darkness and will happen from the dates Tuesday the 16 – Monday the 22 in December. The world will remain, during these three days, without sunlight due to a solar storm, which will cause dust and space debris to become plentiful and thus, block 90% sunlight.\n",
      "\n",
      "\"The head of Nasa Charles Bolden who made the announcement and asked everyone to remain calm. This will be the product of a solar storm, the largest in the last 250 years for a period of 216 hours total.\n",
      "\n",
      "\"Despite the six days of darkness soon to come, officials say that the earth will not experience any major problems, since six days of darkness is nowhere near enough to cause major damage to anything.\"\n",
      "\n",
      "Adding on, the article also carried a made-up quote from Nasa scientist Earl Godoy, saying: \"We will solely rely on artificial light for the six days, which is not a problem at all.\"\n",
      "\n",
      "Many Twitter users believed the fake news report, and expressed their shock.\n",
      "\n",
      "We're going to have a complete 6 days of darkness due to a solar storm in Dec! SO NERVOUS ABOUT THIS! Ahhh. #ThePurge http://t.co/0L2Sis54hv— Janella (@hijanellamarie) October 26, 2014 6 days of total darkness in December? ? http://t.co/eTN60TnXft— Jammie Macaranas (@JammiePeach) October 26, 2014 \"NASA Confirms Earth will experience 6 Days of total DARKNESS in December 2014.\" Me: pic.twitter.com/xZG1xaxqdw— [Hiatus] TT (@sarangBCES) October 26, 2014 \"NASA Confirms Earth Will Experience 6 Days of Total Darkness in December 2014!\" omg what?— 查理 (@Chxrliecutie) October 26, 2014 islam know what this means im scared \"NASA Confirms Earth Will Experience 6 Days of Total Darkness in December 2014! http://t.co/GQGeGLmElZ\"— hiatus (@taobby) October 26, 2014\n",
      "\n",
      "The website has previously published a fake report about American rapper and actor Tupac Shakur, claiming that he is alive.\n",
      "\n",
      "RelatedHalloween 2014 on Friday the 13th for First Time in 666 Years Declared a HoaxShah Rukh Khan's Son Aryan and Aishwarya Rai Bachchan's Niece Navya's Leaked Sex Tape is FakeEbola Zombies: Victims 'Rising from the Dead' Fake News Story Goes Viral, Sparks Outrage on Social MediaEminem 'Quits Music After Checking Into Rehab Again For Heroin Addiction' is Hoax: Satirical Article Creates Stir on Social Media\n",
      "['thousands', 'of', 'people', 'have', 'been', 'duped', 'by', 'a', 'fake', 'news', 'story', 'claiming', 'that', 'nasa', 'has', 'forecast', 'a', 'total', 'blackout', 'of', 'earth', 'for', 'six', 'days', 'in', 'december', 'the', 'story', 'entitled', 'nasa', 'confirms', 'earth', 'will', 'experience', 'days', 'of', 'total', 'darkness', 'in', 'december', 'originated', 'from', 'huzlers', 'com', 'a', 'website', 'well', 'known', 'for', 'publishing', 'fake', 'stories', 'with', 'sensational', 'headlines', 'the', 'bogus', 'report', 'read', 'nasa', 'has', 'confirmed', 'that', 'the', 'earth', 'will', 'experience', 'days', 'of', 'almost', 'complete', 'darkness', 'and', 'will', 'happen', 'from', 'the', 'dates', 'tuesday', 'the', 'monday', 'the', 'in', 'december', 'the', 'world', 'will', 'remain', 'during', 'these', 'three', 'days', 'without', 'sunlight', 'due', 'to', 'a', 'solar', 'storm', 'which', 'will', 'cause', 'dust', 'and', 'space', 'debris', 'to', 'become', 'plentiful', 'and', 'thus', 'block', 'sunlight', 'the', 'head', 'of', 'nasa', 'charles', 'bolden', 'who', 'made', 'the', 'announcement', 'and', 'asked', 'everyone', 'to', 'remain', 'calm', 'this', 'will', 'be', 'the', 'product', 'of', 'a', 'solar', 'storm', 'the', 'largest', 'in', 'the', 'last', 'years', 'for', 'a', 'period', 'of', 'hours', 'total', 'despite', 'the', 'six', 'days', 'of', 'darkness', 'soon', 'to', 'come', 'officials', 'say', 'that', 'the', 'earth', 'will', 'not', 'experience', 'any', 'major', 'problems', 'since', 'six', 'days', 'of', 'darkness', 'is', 'nowhere', 'near', 'enough', 'to', 'cause', 'major', 'damage', 'to', 'anything', 'adding', 'on', 'the', 'article', 'also', 'carried', 'a', 'made', 'up', 'quote', 'from', 'nasa', 'scientist', 'earl', 'godoy', 'saying', 'we', 'will', 'solely', 'rely', 'on', 'artificial', 'light', 'for', 'the', 'six', 'days', 'which', 'is', 'not', 'a', 'problem', 'at', 'all', 'many', 'twitter', 'users', 'believed', 'the', 'fake', 'news', 'report', 'and', 'expressed', 'their', 'shock', 'we', 're', 'going', 'to', 'have', 'a', 'complete', 'days', 'of', 'darkness', 'due', 'to', 'a', 'solar', 'storm', 'in', 'dec', 'so', 'nervous', 'about', 'this', 'ahhh', 'thepurge', 'http', 't', 'co', 'janella', 'hijanellamarie', 'october', 'days', 'of', 'total', 'darkness', 'in', 'december', 'http', 't', 'co', 'jammie', 'macaranas', 'jammiepeach', 'october', 'nasa', 'confirms', 'earth', 'will', 'experience', 'days', 'of', 'total', 'darkness', 'in', 'december', 'me', 'pic', 'twitter', 'com', 'hiatus', 'tt', 'sarangbces', 'october', 'nasa', 'confirms', 'earth', 'will', 'experience', 'days', 'of', 'total', 'darkness', 'in', 'december', 'omg', 'what', '查理', 'chxrliecutie', 'october', 'islam', 'know', 'what', 'this', 'means', 'im', 'scared', 'nasa', 'confirms', 'earth', 'will', 'experience', 'days', 'of', 'total', 'darkness', 'in', 'december', 'http', 't', 'co', 'gqgeglmelz', 'hiatus', 'taobby', 'october', 'the', 'website', 'has', 'previously', 'published', 'a', 'fake', 'report', 'about', 'american', 'rapper', 'and', 'actor', 'tupac', 'shakur', 'claiming', 'that', 'he', 'is', 'alive', 'relatedhalloween', 'on', 'friday', 'the', 'for', 'first', 'time', 'in', 'years', 'declared', 'a', 'hoaxshah', 'rukh', 'khan', 's', 'son', 'aryan', 'and', 'aishwarya', 'rai', 'bachchan', 's', 'niece', 'navya', 's', 'leaked', 'sex', 'tape', 'is', 'fakeebola', 'zombies', 'victims', 'rising', 'from', 'the', 'dead', 'fake', 'news', 'story', 'goes', 'viral', 'sparks', 'outrage', 'on', 'social', 'mediaeminem', 'quits', 'music', 'after', 'checking', 'into', 'rehab', 'again', 'for', 'heroin', 'addiction', 'is', 'hoax', 'satirical', 'article', 'creates', 'stir', 'on', 'social', 'media']\n",
      "['isis', 'video', 'america', 's', 'air', 'dropped', 'weapons', 'now', 'in', 'our', 'hands']\n",
      "discuss\n"
     ]
    }
   ],
   "source": [
    "print(train_data_articles_df['Article'][154])\n",
    "print(train_data_articles_df['article_cleaned'][154])\n",
    "print(train_data_stances_df['headline_cleaned'][154])\n",
    "print(train_data_stances_df['Stance'][154])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8433fa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "website_words = ['http', 'twitter', 'com', 'pic', 'co']\n",
    "month_words = ['january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', 'novemeber', 'december']\n",
    "\n",
    "my_banned_words = stopwords.words('english') + website_words + month_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d1c666b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(string):\n",
    "    output = []\n",
    "    for word in string:\n",
    "        if word not in my_banned_words:\n",
    "            output.append(word)\n",
    "    return output\n",
    "\n",
    "train_data_articles_df['article_cleaned'] =  train_data_articles_df['article_cleaned'].apply(lambda x: remove_stopwords(x))\n",
    "train_data_stances_df['headline_cleaned'] =  train_data_stances_df['headline_cleaned'].apply(lambda x: remove_stopwords(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c828b2d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thousands', 'people', 'duped', 'fake', 'news', 'story', 'claiming', 'nasa', 'forecast', 'total', 'blackout', 'earth', 'six', 'days', 'story', 'entitled', 'nasa', 'confirms', 'earth', 'experience', 'days', 'total', 'darkness', 'originated', 'huzlers', 'website', 'well', 'known', 'publishing', 'fake', 'stories', 'sensational', 'headlines', 'bogus', 'report', 'read', 'nasa', 'confirmed', 'earth', 'experience', 'days', 'almost', 'complete', 'darkness', 'happen', 'dates', 'tuesday', 'monday', 'world', 'remain', 'three', 'days', 'without', 'sunlight', 'due', 'solar', 'storm', 'cause', 'dust', 'space', 'debris', 'become', 'plentiful', 'thus', 'block', 'sunlight', 'head', 'nasa', 'charles', 'bolden', 'made', 'announcement', 'asked', 'everyone', 'remain', 'calm', 'product', 'solar', 'storm', 'largest', 'last', 'years', 'period', 'hours', 'total', 'despite', 'six', 'days', 'darkness', 'soon', 'come', 'officials', 'say', 'earth', 'experience', 'major', 'problems', 'since', 'six', 'days', 'darkness', 'nowhere', 'near', 'enough', 'cause', 'major', 'damage', 'anything', 'adding', 'article', 'also', 'carried', 'made', 'quote', 'nasa', 'scientist', 'earl', 'godoy', 'saying', 'solely', 'rely', 'artificial', 'light', 'six', 'days', 'problem', 'many', 'users', 'believed', 'fake', 'news', 'report', 'expressed', 'shock', 'going', 'complete', 'days', 'darkness', 'due', 'solar', 'storm', 'dec', 'nervous', 'ahhh', 'thepurge', 'janella', 'hijanellamarie', 'days', 'total', 'darkness', 'jammie', 'macaranas', 'jammiepeach', 'nasa', 'confirms', 'earth', 'experience', 'days', 'total', 'darkness', 'hiatus', 'tt', 'sarangbces', 'nasa', 'confirms', 'earth', 'experience', 'days', 'total', 'darkness', 'omg', '查理', 'chxrliecutie', 'islam', 'know', 'means', 'im', 'scared', 'nasa', 'confirms', 'earth', 'experience', 'days', 'total', 'darkness', 'gqgeglmelz', 'hiatus', 'taobby', 'website', 'previously', 'published', 'fake', 'report', 'american', 'rapper', 'actor', 'tupac', 'shakur', 'claiming', 'alive', 'relatedhalloween', 'friday', 'first', 'time', 'years', 'declared', 'hoaxshah', 'rukh', 'khan', 'son', 'aryan', 'aishwarya', 'rai', 'bachchan', 'niece', 'navya', 'leaked', 'sex', 'tape', 'fakeebola', 'zombies', 'victims', 'rising', 'dead', 'fake', 'news', 'story', 'goes', 'viral', 'sparks', 'outrage', 'social', 'mediaeminem', 'quits', 'music', 'checking', 'rehab', 'heroin', 'addiction', 'hoax', 'satirical', 'article', 'creates', 'stir', 'social', 'media']\n"
     ]
    }
   ],
   "source": [
    "print(train_data_articles_df['article_cleaned'][154])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1b80325",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('processed'):\n",
    "    os.mkdir('processed')\n",
    "\n",
    "train_data_articles_df.to_csv(\"processed/processed_train_articles.csv\")\n",
    "train_data_stances_df.to_csv(\"processed/processed_train_stances.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8969de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting training/test sets\n",
    "print(train_data_articles_df.shape)\n",
    "print(train_data_stances_df.shape)\n",
    "\n",
    "test_ids = []\n",
    "with open('splits/hold_out_ids.txt') as f: # change the random seed in the utils file\n",
    "    for line in f:\n",
    "        test_ids.append(int(line.rstrip()))\n",
    "\n",
    "test_articles_dfs = []\n",
    "test_stances_dfs = []\n",
    "for id in test_ids:\n",
    "    article_df = train_data_articles_df[train_data_articles_df['Body ID'] == id]\n",
    "    article_index = int(article_df.index[0])\n",
    "    test_articles_dfs.append(article_df)\n",
    "    train_data_articles_df = train_data_articles_df.drop(article_index)\n",
    "    \n",
    "    stance_df = train_data_stances_df[train_data_stances_df['Body ID'] == id]\n",
    "    test_stances_dfs.append(stance_df)\n",
    "    for stance_index in stance_df.index.values:\n",
    "        train_data_stances_df.drop(int(stance_index))\n",
    "\n",
    "test_data_articles_df = pd.concat(test_articles_dfs)\n",
    "test_data_stances_df = pd.concat(test_stances_dfs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d246eb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_data_articles_df.shape)\n",
    "print(train_data_articles_df.shape)\n",
    "print(test_data_stances_df.shape)\n",
    "print(train_data_stances_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d054dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0  1787  3974  4936  5210  5863  6199  6756  7526  9003 10036 10780\n",
      " 11687 11864 15746 21620 21712 21928 22100 25006 25492 25616 26260 26398\n",
      " 27200 29988 33683 37095 38326 41035 42776 43897 44978 45222 45579 46530\n",
      " 47712 47850 48228]\n"
     ]
    }
   ],
   "source": [
    "stance_df = train_data_stances_df[train_data_stances_df['Body ID'] == 712]\n",
    "print(stance_df.index.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f7976c",
   "metadata": {},
   "source": [
    "\n",
    " Strategy: concatenate headline with article, encode as bag of words, tfidf score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8be437a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stance_to_number = {\n",
    "    \"agree\": 0,\n",
    "    \"disagree\": 1,\n",
    "    \"discuss\": 2,\n",
    "    \"unrelated\": 3\n",
    "}\n",
    "\n",
    "train_ids = []\n",
    "test_ids = []\n",
    "\n",
    "with open('splits/training_ids.txt') as f: # change the random seed in the utils file\n",
    "    for line in f:\n",
    "        train_ids.append(int(line.rstrip()))\n",
    "with open('splits/hold_out_ids.txt') as f: # change the random seed in the utils file\n",
    "    for line in f:\n",
    "        test_ids.append(int(line.rstrip()))\n",
    "        \n",
    "def concat_data(articles_df, headlines_df, body_ids):\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    for body_id in body_ids:\n",
    "        article = articles_df[articles_df['Body ID'] == body_id]['article_cleaned'].values[0]\n",
    "    #     print(article)\n",
    "        headlines = headlines_df[headlines_df['Body ID'] == body_id]['headline_cleaned'].values\n",
    "    #     print(headlines)\n",
    "        stances = headlines_df[headlines_df['Body ID'] == body_id]['Stance'].values\n",
    "    #     print(stances)\n",
    "        for headline, stance in zip(headlines, stances):\n",
    "            features.append(headline+article)\n",
    "            labels.append(stance_to_number[stance])\n",
    "    return features, labels\n",
    "\n",
    "train_features, train_labels = concat_data(train_data_articles_df, train_data_stances_df, train_ids)\n",
    "test_features, test_labels = concat_data(train_data_articles_df, train_data_stances_df, test_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7763744f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40350\n"
     ]
    }
   ],
   "source": [
    "print(len(train_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f65430d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40350, 20333)\n",
      "11881\n",
      "(9622, 11881)\n",
      "['_dpaj', 'aamir', 'aan', 'aapl', 'aaron', 'aback', 'abadam', 'abadi', 'abagnale', 'abandoned']\n"
     ]
    }
   ],
   "source": [
    "# dummy preprocessor and tokenizer because already did that above. not going to change it up to fit this format\n",
    "def dummy(doc):\n",
    "    return doc\n",
    "vectorizer = CountVectorizer(preprocessor=dummy, tokenizer=dummy)\n",
    "train_features_counts = vectorizer.fit_transform(train_features)\n",
    "test_features_counts = vectorizer.fit_transform(test_features)\n",
    "\n",
    "print(train_features_counts.shape)\n",
    "print(len(vectorizer.get_feature_names()))\n",
    "print(test_features_counts.shape)\n",
    "print(vectorizer.get_feature_names()[0:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5634b676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40350, 20333)\n"
     ]
    }
   ],
   "source": [
    "train_tfidf_transformer = TfidfTransformer()\n",
    "train_features_tfidf = train_tfidf_transformer.fit_transform(train_features_counts)\n",
    "\n",
    "test_tfidf_transformer = TfidfTransformer()\n",
    "test_features_tfidf = test_tfidf_transformer.fit_transform(test_features_counts)\n",
    "\n",
    "print(train_features_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "feae8c58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_naive_bayes = MultinomialNB()\n",
    "model_naive_bayes.fit(train_features_tfidf, train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fc885561",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 11881 features, but MultinomialNB is expecting 20333 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [74]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model_naive_bayes_predicted \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_naive_bayes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_features_tfidf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(model_naive_bayes_predicted \u001b[38;5;241m==\u001b[39m test_labels)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(accuracy)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu-1\\lib\\site-packages\\sklearn\\naive_bayes.py:82\u001b[0m, in \u001b[0;36m_BaseNB.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;124;03mPerform classification on an array of test vectors X.\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;124;03m    Predicted target values for X.\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     81\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m---> 82\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_X\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m jll \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_joint_log_likelihood(X)\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_[np\u001b[38;5;241m.\u001b[39margmax(jll, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu-1\\lib\\site-packages\\sklearn\\naive_bayes.py:519\u001b[0m, in \u001b[0;36m_BaseDiscreteNB._check_X\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    517\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_X\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    518\u001b[0m     \u001b[38;5;124;03m\"\"\"Validate X, used only in predict* methods.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 519\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu-1\\lib\\site-packages\\sklearn\\base.py:585\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    582\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 585\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    587\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu-1\\lib\\site-packages\\sklearn\\base.py:400\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 400\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    401\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    402\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    403\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 11881 features, but MultinomialNB is expecting 20333 features as input."
     ]
    }
   ],
   "source": [
    "model_naive_bayes_predicted = model_naive_bayes.predict(test_features_tfidf)\n",
    "accuracy = np.mean(model_naive_bayes_predicted == test_labels)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd308499",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu-1",
   "language": "python",
   "name": "gpu-1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
